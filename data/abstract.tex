% !TeX root = ../thuthesis-example.tex

% 中英文摘要和关键字

\begin{abstract}
  随着自动驾驶技术向L5级完全自主驾驶演进，高保真场景重建已从辅助感知工具转变为支撑整个研发体系的核心技术基础。在大规模仿真测试体系中，视点外推能力是实现灵活轨迹规划和多样化场景评估的关键技术瓶颈。现有神经场景表示方法在处理训练视点分布之外的新视角渲染时，普遍面临几何一致性退化和视觉质量下降的系统性问题。本文针对4D高斯溅射（4D Gaussian Splatting, 4DGS）在视点泛化能力上的根本性局限，提出了一种基于扩散先验的引导优化框架ReGen，通过系统性地融合视频扩散模型的生成先验知识来增强4DGS在未见视点上的渲染质量和几何一致性。

  本文的技术贡献体现在方法论创新、架构设计和系统实现三个层面。在方法论创新层面，提出了训练自由引导（Training-Free Guidance）机制，该机制通过同时利用LiDAR几何条件和当前4DGS渲染状态构建双重引导策略，将扩散模型的去噪过程显式地分解为条件引导和自由引导两个正交分量，实现了更加精确和稳定的监督信号传递。设计了自适应采样调度（Adaptive Sampling Scheduling）算法，根据4DGS模型的收敛状态和渲染质量动态调整扩散采样的频率和去噪强度，在训练早期提供强引导以建立正确的几何结构，在训练后期减弱引导以保持模型的泛化能力，显著提升了训练效率和优化质量。构建了多模态正则化框架，综合考虑几何一致性、时间连贯性和视觉质量三个维度，通过深度一致性正则项、光流平滑性约束和感知损失的协同作用，确保4DGS在优化过程中保持多视角几何结构的全局一致性和时序动态的平滑连贯性。

  在架构设计层面，提出了五层解耦的系统架构，包含数据抽象层、几何表示层、神经渲染层、扩散推理层和扩散监督层。数据抽象层负责输入数据的统一格式化和预处理；几何表示层采用静态背景、动态物体和环境映射的三元组合表示，支持大规模城市级场景的高效建模；神经渲染层实现了tile-based光栅化引擎，通过GPU内存池管理和自适应排序优化实现超过100帧每秒的实时渲染；扩散推理层封装了基于DiT架构的视频扩散模型，集成了滑动窗口机制和键值缓存技术以支持长序列生成；扩散监督层协调两个模型的联合优化，实现了内存高效的异步计算流水线。这种分层设计降低了系统复杂度，支持组件级的独立优化和灵活替换，为大规模系统的工程实现提供了坚实的架构基础。

  在系统实现层面，采用了基于Wan2.2-I2V-A14B的Diffusion Transformer（DiT）架构。该架构通过混合专家（Mixture-of-Experts, MoE）机制实现了基于信噪比的自适应专家路由，高噪声专家负责整体布局和动态构建，低噪声专家专注于细节优化，显著提升了视频生成的质量和效率。针对长视频序列处理的内存挑战，设计了50\%重叠的滑动窗口策略和跨窗口键值缓存机制，在保持长程时空依赖建模能力的同时有效控制了显存占用。实现了融合FSDP（Fully Sharded Data Parallel）和Ulysses序列并行的分布式训练架构，通过参数分片、梯度压缩和动态负载均衡策略，支持在多GPU集群上进行大规模模型的高效训练。设计了多模式推理引擎，支持单GPU和多GPU两种部署方式，通过模型权重卸载、数据类型转换和批处理优化，实现了灵活的性能与资源权衡。

  由于内部数据涉及保密要求，本文使用公开数据集进行实验验证，在视点外推任务上相比现有最先进方法取得了显著的性能提升，同时保持了实时渲染的计算效率。系统性的消融实验验证了各个技术组件的有效性和必要性，定性分析展示了方法在复杂城市场景中的渲染质量和几何一致性。需要说明的是，为满足学术发表的公开性要求，论文中部分技术细节已进行适当处理。研究成果为高保真场景重建技术在自动驾驶仿真中的实际应用提供了完整的理论框架、系统架构和工程实现方案，对推动自动驾驶技术向L5级完全自主驾驶演进具有重要的理论意义和实用价值。

  % 关键词用"英文逗号"分隔，输出时会自动处理为正确的分隔符
  \thusetup{
    keywords = {自动驾驶, 场景重建, 4D高斯溅射, 视频扩散模型, 扩散监督训练, 视点外推},
  }
\end{abstract}

\begin{abstract*}
  As autonomous driving technology advances toward Level 5 full autonomy, high-fidelity scene reconstruction has evolved from a perception assistance tool to a core technical foundation supporting the entire development ecosystem. In large-scale simulation testing frameworks, novel view extrapolation capability represents a critical technical bottleneck for achieving flexible trajectory planning and diverse scenario evaluation. Existing neural scene representation methods universally face systematic challenges of geometric consistency degradation and visual quality deterioration when rendering novel viewpoints beyond the training view distribution. This dissertation addresses the fundamental limitations of 4D Gaussian Splatting (4DGS) in view generalization by proposing ReGen, a diffusion-prior-based guided optimization framework that systematically integrates the generative prior knowledge of video diffusion models to enhance 4DGS's rendering quality and geometric consistency at unseen viewpoints.

  The technical contributions of this work are manifested across three levels: methodological innovation, architectural design, and system implementation. At the methodological innovation level, we propose a Training-Free Guidance mechanism that constructs a dual-guidance strategy by simultaneously leveraging LiDAR geometric conditions and current 4DGS rendering states, explicitly decomposing the denoising process of the diffusion model into conditional guidance and free guidance as two orthogonal components, achieving more precise and stable supervision signal propagation. We design an Adaptive Sampling Scheduling algorithm that dynamically adjusts the frequency and denoising strength of diffusion sampling based on the convergence state and rendering quality of the 4DGS model, providing strong guidance in early training stages to establish correct geometric structures while reducing guidance in later stages to maintain model generalization capability, significantly improving training efficiency and optimization quality. We construct a multi-modal regularization framework that comprehensively considers three dimensions—geometric consistency, temporal coherence, and visual quality—ensuring that 4DGS maintains global consistency of multi-view geometric structures and smooth coherence of temporal dynamics during optimization through the synergistic effects of depth consistency regularization, optical flow smoothness constraints, and perceptual losses.

  At the architectural design level, we propose a five-layer decoupled system architecture comprising data abstraction, geometric representation, neural rendering, diffusion inference, and diffusion supervision layers. The data abstraction layer handles unified formatting and preprocessing of input data; the geometric representation layer adopts a tripartite composition of static backgrounds, dynamic objects, and environment mapping to support efficient modeling of large-scale city-level scenes; the neural rendering layer implements a tile-based rasterization engine achieving real-time rendering exceeding 100 frames per second through GPU memory pool management and adaptive sorting optimization; the diffusion inference layer encapsulates a DiT architecture-based video diffusion model, integrating sliding window mechanisms and key-value caching techniques to support long-sequence generation; the diffusion supervision layer coordinates joint optimization of both models, implementing a memory-efficient asynchronous computation pipeline. This layered design reduces system complexity, supports component-level independent optimization and flexible replacement, providing a solid architectural foundation for large-scale system engineering implementation.

  At the system implementation level, we adopt a Diffusion Transformer (DiT) architecture based on Wan2.2-I2V-A14B. This architecture achieves adaptive expert routing based on signal-to-noise ratios through Mixture-of-Experts (MoE) mechanisms, where high-noise experts handle overall layout and dynamics construction while low-noise experts focus on detail refinement, significantly improving video generation quality and efficiency. To address memory challenges in long video sequence processing, we design a 50\% overlapping sliding window strategy with cross-window key-value caching mechanisms, effectively controlling memory footprint while maintaining long-range spatiotemporal dependency modeling capabilities. We implement a distributed training architecture integrating FSDP (Fully Sharded Data Parallel) and Ulysses sequence parallelism, supporting efficient training of large-scale models on multi-GPU clusters through parameter sharding, gradient compression, and dynamic load balancing strategies. We design a multi-mode inference engine supporting both single-GPU and multi-GPU deployment modes, achieving flexible performance-resource trade-offs through model weight offloading, data type conversion, and batch processing optimization.

  Due to confidentiality requirements of internal data, we conduct experimental validation using publicly available datasets, achieving significant performance improvements in novel view extrapolation tasks compared to state-of-the-art methods while maintaining real-time rendering computational efficiency. Systematic ablation experiments validate the effectiveness and necessity of each technical component, while qualitative analyses demonstrate the method's rendering quality and geometric consistency in complex urban scenarios. It should be noted that certain technical details have been appropriately adapted to meet the public disclosure requirements for academic publications. The research outcomes provide a complete theoretical framework, system architecture, and engineering implementation solution for high-fidelity scene reconstruction technology in autonomous driving simulation applications, holding significant theoretical importance and practical value for advancing autonomous driving technology toward Level 5 full autonomy.

  % Use comma as separator when inputting
  \thusetup{
    keywords* = {Autonomous Driving, Scene Reconstruction, 4D Gaussian Splatting, Video Diffusion Model, Diffusion-Supervised Training, Novel View Synthesis},
  }
\end{abstract*}
