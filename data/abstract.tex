% !TeX root = ../thuthesis-example.tex

% 中英文摘要和关键字

\begin{abstract}
  随着自动驾驶技术向L5级完全自主驾驶演进，高保真场景重建在大规模仿真测试中发挥着核心作用。视点外推能力是实现灵活轨迹规划和多样化场景评估的关键技术瓶颈。现有神经场景表示方法在渲染训练视点分布之外的新视角时，普遍面临几何一致性退化和视觉质量下降的问题。本文针对4D高斯溅射（4DGS）在视点泛化能力上的局限，提出了基于扩散先验的引导优化框架ReGen，通过融合视频扩散模型的生成先验知识来增强4DGS在未见视点上的渲染质量和几何一致性。

  本文的技术贡献体现在方法论创新、架构设计和系统实现三个层面。在方法论层面，提出了训练自由引导机制，通过LiDAR几何条件和4DGS渲染状态构建双重引导策略，将扩散去噪过程分解为条件引导和自由引导两个正交分量，实现精确稳定的监督信号传递。设计了自适应采样调度算法，根据模型收敛状态动态调整扩散采样频率和强度，在训练早期提供强引导建立几何结构，后期减弱引导保持泛化能力。构建了多模态正则化框架，通过深度一致性、光流平滑性和感知损失的协同作用，确保多视角几何结构的全局一致性和时序动态的平滑连贯性。

  在架构设计层面，提出了五层解耦的系统架构：数据抽象层、几何表示层、神经渲染层、扩散推理层和扩散监督层。几何表示层采用静态背景、动态物体和环境映射的三元组合表示；神经渲染层实现了tile-based光栅化引擎，实现实时渲染；扩散推理层封装了基于DiT架构的视频扩散模型，集成滑动窗口和键值缓存技术支持长序列生成；扩散监督层协调两个模型的联合优化，实现内存高效的异步计算流水线。

  在系统实现层面，采用基于Wan2.2-I2V-A14B的Diffusion Transformer架构。该架构通过混合专家（MoE）机制实现自适应专家路由，高噪声专家负责整体布局，低噪声专家专注细节优化。针对内存挑战，设计了滑动窗口策略和键值缓存机制。实现了融合FSDP和Ulysses序列并行的分布式训练架构，通过参数分片、梯度压缩和动态负载均衡，支持多GPU集群上的高效训练。

  需要说明的是，由于内部数据和算法涉及保密要求，本文使用Waymo Open Dataset等公开数据集进行实验验证，算法实现为简化的早期研究版本，非量产系统。实验结果表明，在新视角外推任务上相比现有方法取得了显著性能提升，消融实验验证了各技术组件的有效性。研究成果为高保真场景重建技术在自动驾驶仿真中的应用提供了理论框架、系统架构和工程实现方案。

  % 关键词用"英文逗号"分隔，输出时会自动处理为正确的分隔符
  \thusetup{
    keywords = {自动驾驶, 场景重建, 4D高斯溅射, 视频扩散模型, 扩散监督训练, 视点外推},
  }
\end{abstract}

\begin{abstract*}
  As autonomous driving technology advances toward Level 5 full autonomy, high-fidelity scene reconstruction plays a core role in large-scale simulation testing. Novel view extrapolation capability represents a critical technical bottleneck for flexible trajectory planning and diverse scenario evaluation. Existing neural scene representation methods face challenges of geometric consistency degradation and visual quality deterioration when rendering viewpoints beyond the training distribution. This dissertation addresses the limitations of 4D Gaussian Splatting (4DGS) in view generalization by proposing ReGen, a diffusion-prior-based guided optimization framework that integrates video diffusion models' generative prior knowledge to enhance 4DGS rendering quality and geometric consistency at unseen viewpoints.

  The technical contributions span three levels: methodological innovation, architectural design, and system implementation. At the methodological level, we propose a Training-Free Guidance mechanism that constructs a dual-guidance strategy using LiDAR geometric conditions and 4DGS rendering states, decomposing the diffusion denoising process into conditional and free guidance components for precise supervision signal propagation. We design an Adaptive Sampling Scheduling algorithm that dynamically adjusts diffusion sampling frequency and strength based on model convergence state, providing strong guidance early to establish geometric structures while reducing guidance later to maintain generalization. We construct a multi-modal regularization framework ensuring global consistency of multi-view geometric structures and smooth temporal dynamics through depth consistency, optical flow smoothness, and perceptual losses.

  At the architectural level, we propose a five-layer decoupled system architecture: data abstraction, geometric representation, neural rendering, diffusion inference, and diffusion supervision layers. The geometric representation layer adopts a tripartite composition of static backgrounds, dynamic objects, and environment mapping. The neural rendering layer implements a tile-based rasterization engine achieving real-time rendering. The diffusion inference layer encapsulates a DiT-based video diffusion model with sliding window and key-value caching for long-sequence generation. The diffusion supervision layer coordinates joint optimization with memory-efficient asynchronous computation.

  At the implementation level, we adopt a Diffusion Transformer architecture based on Wan2.2-I2V-A14B. The architecture achieves adaptive expert routing through Mixture-of-Experts (MoE) mechanisms, with high-noise experts handling overall layout and low-noise experts focusing on detail refinement. To address memory challenges, we design sliding window strategies and key-value caching mechanisms. We implement distributed training integrating FSDP and Ulysses sequence parallelism through parameter sharding, gradient compression, and dynamic load balancing.

  It should be noted that due to confidentiality requirements, we conduct experimental validation using publicly available datasets such as Waymo Open Dataset, and the algorithm implementation represents a simplified early research version rather than a production system. Experimental results demonstrate significant performance improvements in novel view extrapolation compared to existing methods. Ablation experiments validate the effectiveness of each technical component. The research provides a theoretical framework, system architecture, and engineering implementation solution for high-fidelity scene reconstruction in autonomous driving simulation.

  % Use comma as separator when inputting
  \thusetup{
    keywords* = {Autonomous Driving, Scene Reconstruction, 4D Gaussian Splatting, Video Diffusion Model, Diffusion-Supervised Training, Novel View Synthesis},
  }
\end{abstract*}
