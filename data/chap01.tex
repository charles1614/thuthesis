% !TeX root = ../thuthesis-example.tex

\chapter{引言}

\section{研究背景与意义}

自动驾驶技术作为人工智能与交通运输领域深度融合的革命性产物，正以前所未有的速度重塑着人类的出行方式与社会结构。其核心目标是构建能够完全替代人类驾驶员的智能系统，以期在提升交通安全、优化通行效率、降低能源消耗等方面取得突破性进展。然而，实现完全自动驾驶（L5级别）的道路依旧充满挑战，其中最核心的瓶颈之一在于如何确保系统在无穷尽的复杂、动态甚至极端交通场景中的安全性和鲁棒性。

为了应对这一挑战，大规模、高质量、高多样性的驾驶数据成为训练和验证自动驾驶算法的基石。在此背景下，高保真度的自动驾驶仿真环境应运而生，并迅速从辅助性测试工具演变为不可或缺的核心研发平台。一个先进的仿真系统不仅是算法可视化的窗口，更是贯穿自动驾驶技术整个生命周期的关键环节。它能够以极低的成本和零风险的方式，为自动驾驶系统提供闭环评估环境，并大规模生成在真实世界中难以采集到的安全关键"长尾"场景（corner-case data）\cite{chen2025snerf}。

随着行业技术路线逐渐向端到端（end-to-end）模型演进，仿真环境的重要性被进一步放大。这些模型直接将传感器输入映射到车辆控制指令，传统的开环日志回放（log replay）测试方法已无法满足其验证需求，因为系统无法与环境进行实时交互。因此，构建一个能够对智能体的行为做出动态响应、无"仿真到现实"（sim-to-real）领域差异的交互式仿真环境，已成为推动自动驾驶技术发展的迫切需求\cite{lin2024drivinggaussian}。

基于学习的三维场景重建技术，正是构建此类下一代仿真器的核心与基础\cite{tonderski2024neurad}。这种需求背后也蕴含着深刻的经济与可扩展性动因。在真实世界中进行数据采集，尤其是针对罕见但至关重要的长尾事件，不仅成本高昂、耗时费力，还伴随着潜在的安全风险\cite{yan2024street}。一个能够逼真重建并渲染真实世界场景的仿真平台，可以通过数据合成的方式，有效规避物理采集过程的种种限制，从而极大地降低研发成本，加速算法的迭代周期\cite{chen2025snerf}。因此，神经渲染与场景重建技术的每一次进步，都不再仅仅是计算机图形学领域的学术探索，而是直接解决了自动驾驶产业化进程中的一个根本性经济瓶颈。

从技术演进的视角看，端到端模型的兴起也对仿真技术提出了更高的要求。传统的模块化系统（感知、预测、规划）尚可在一定程度上通过日志回放进行开环测试。然而，端到端模型输出的每一个控制指令（如转向）都会即刻改变下一帧所需的传感器输入。日志数据无法提供这种因果交互下的新视角数据，必须依赖一个可动态渲染的三维场景表示来实时生成世界对智能体行为的反馈。这一转变将技术需求从简单的"新视角合成"（novel view synthesis）提升到了"可控的、可交互的场景实时渲染"，后者是一个远比前者更为艰巨的挑战，也正是本论文所致力于解决的核心问题。

\section{自动驾驶场景重建技术现状与挑战}

近年来，以数据驱动的神经场景重建技术取得了长足发展，深刻改变了高保真仿真环境的构建范式。其中，两条主流技术路线——神经辐射场（NeRF）与三维高斯溅射（3DGS），共同定义了当前领域的技术前沿，也各自带来了新的挑战。

\subsection{神经辐射场时代：范式革新与性能瓶颈}

神经辐射场（NeRF）的出现，是神经渲染领域的一次范式革命\cite{mildenhall2021nerf}。它通过一个多层感知机（MLP）学习一个连续的体积函数，将五维坐标（三维空间位置$(x, y, z)$和二维观测方向$(\theta, \phi)$）映射到体密度和颜色值。通过可微分的体积渲染技术，NeRF能够仅从一组二维图像及其对应的相机位姿中，重建出具有前所未有照片级真实感的复杂三维场景。这一突破迅速激发了学术界将其应用于自动驾驶场景重建的热情，催生了如S-NeRF++、Urban Radiance Field、NeuRAD等一系列优秀工作\cite{chen2025snerf}。这些方法通常会融合激光雷达（LiDAR）等其他传感器数据作为几何先验，以提升在大尺度、无界城市场景中的重建精度和几何准确性\cite{lin2024drivinggaussian}。

然而，NeRF的成功背后也潜藏着一个致命的弱点：其渲染过程的计算成本极为高昂。由于需要沿着每条渲染光线密集采样数百个点，并逐一通过神经网络进行前向传播，NeRF的渲染速度远未达到实时水平\cite{lin2024drivinggaussian}。这种固有的计算效率低下，使其难以满足自动驾驶闭环仿真等需要即时反馈的应用场景，构成了其通往工业级应用的主要障碍\cite{lin2024drivinggaussian}。

\subsection{三维高斯溅射的兴起：实时渲染与新的难题}

为了突破NeRF的性能瓶颈，三维高斯溅射（3D Gaussian Splatting, 3DGS）作为一种新型的显式场景表示方法应运而生，并迅速成为实时渲染应用的首选方案\cite{kerbl2023gaussian}。3DGS使用一组三维高斯椭球体来显式地表示场景，每个高斯体都包含位置、协方差（形状与旋转）、颜色和不透明度等属性\cite{kerbl2023gaussian}。借助高效的可微分光栅化渲染管线，3DGS成功地将体积渲染的高质量与传统光栅化的超高速度相结合，能够以超过30帧每秒（fps）的速率实时渲染高分辨率图像，达到了交互式应用的关键门槛\cite{sun2025splatflow}。

然而，3DGS在解决渲染速度问题的同时，也引入了新的挑战。首先，原始的3DGS主要为静态场景设计，直接应用于包含大量运动物体的动态驾驶场景时效果不佳\cite{sun2025splatflow}。其次，作为一种纯粹的重建方法，3DGS的渲染质量高度依赖于训练视角的覆盖范围。当渲染一个远离训练相机轨迹的新视角时，其生成的图像质量会急剧下降，出现模糊、伪影等问题，这一现象被称为"视角外推"（view extrapolation）能力差\cite{yan2024street}。这极大地限制了其在模拟全新驾驶路径或智能体行为时的应用价值。

从NeRF到3DGS的技术演进，本质上是计算机图形学领域中"隐式表示"与"显式表示"经典权衡的一次体现。NeRF作为隐式表示，其神经网络结构赋予了它强大的表达能力，能够从稀疏的图像中学习到平滑且细节丰富的场景几何，但代价是查询（即渲染）场景中任意一点的属性时需要昂贵的网络评估\cite{mildenhall2021nerf}。而3DGS作为显式表示，将场景分解为一系列独立的图元（高斯体），使得渲染过程可以被高效地并行化和光栅化，从而实现实时性能\cite{kerbl2023gaussian}。这种显式表示的"代价"在于，它更像是一种对观测数据的"插值"，在没有强大的生成先验支持下，很难合理地"推断"出场景中未被观测到的部分。

这一洞察精准地定位了本论文的研究起点：寻求一种能够融合二者之长的方法——既拥有显式表示（3DGS）的渲染效率，又具备强大生成模型（如扩散模型）所赋予的先验知识与泛化能力。

\section{新兴生成模型驱动的研究范式}

在场景重建技术自身演进的同时，另一场深刻的变革正在生成式人工智能领域发生。以降噪扩散模型（Denoising Diffusion Models）为代表的新一代生成模型，凭借其在生成高保真、高多样性内容方面的卓越能力，已成为图像、视频乃至三维内容生成领域的最新技术标杆\cite{ho2020denoising}。

扩散模型通过模拟一个逐步向数据添加高斯噪声的前向过程，并训练一个神经网络来学习其逆过程——即从纯噪声中逐步恢复出原始数据，从而精准地学习到复杂的数据分布\cite{ho2020denoising}。学术界正积极探索如何将这些在二维图像领域取得巨大成功的强大先验知识，迁移应用于三维内容的生成与编辑任务中\cite{ho2020denoising}。

然而，这一跨维度应用的"最后一公里"并非坦途。一个核心挑战在于如何保证生成内容的三维一致性。由于二维扩散模型本身缺乏对三维几何的内在理解，若将其独立地应用于场景的不同视角，极易导致纹理闪烁、几何结构错位等空间不一致的伪影，严重影响重建的三维模型的质量\cite{rossle2024l3dg}。

这一挑战催生了一种新的研究思路：将扩散模型视为一个强大的"想象力引擎"，用以增强而非取代传统的三维重建方法。这标志着一个从纯粹的场景"重建"（reconstruction）到"重建与合理生成相结合"（reconstruction augmented by plausible generation）的根本性范式转变。

传统的重建方法，无论是NeRF还是3DGS，其能力上限均受限于输入视图中所包含的信息，它们只能重建出"所见"的内容。而扩散模型通过在海量互联网级别的图像视频数据上进行预训练，已经内化了关于真实世界"应有样貌"的强大先验知识\cite{yan2024street}。当二者结合时，3DGS负责从真实的传感器数据中构建场景的几何骨架，而扩散模型则负责在骨架之上"填补"未观测区域的空白，为稀疏或模糊的区域"脑补"出合理的细节，并从整体上提升渲染的真实感。这种协同作用，使得最终生成的虚拟环境不再仅仅是输入数据的一个三维拷贝，而是一个语义上更完整、视觉上更丰富的数字孪生。这对于需要模拟各种未知情况的自动驾驶测试而言，其价值不言而喻。

\section{本文研究动机与核心问题}

通过对现有技术的梳理与分析，我们可以清晰地看到一个由两大技术趋势交汇而形成的研究缺口。一方面，以3DGS为代表的显式场景表示技术，解决了自动驾驶仿真所需的实时渲染难题，但在视角外推和内容生成方面表现出明显短板\cite{yan2024street}。另一方面，以扩散模型为代表的生成技术，具备无与伦比的照片级真实感内容生成能力，但本身缺乏显式的三维结构，生成过程缓慢，且无法从任意视点进行实时渲染\cite{yan2024street}。

这种鲜明的技术二元性构成了本研究的核心动机：3DGS拥有"骨架"（高效的实时渲染结构），但缺乏"血肉"（高质量的生成与外推能力）；扩散模型拥有"血肉"，但缺乏"骨架"。

因此，本论文的核心问题可以被精确地表述为：如何将三维高斯溅射（3DGS）高效的显式三维表示，与扩散模型强大的生成先验进行协同融合，构建一个统一的、能够对动态自动驾驶场景进行实时、高保真、几何一致的重建与渲染框架，尤其是在处理远离原始采集轨迹的新视角合成任务时，能够展现出卓越的性能？

该问题陈述直接源于上述分析所识别出的技术鸿沟，为本论文设定了一个清晰且富有挑战性的研究目标，旨在突破当前技术的局限，推动自动驾驶仿真技术向更高水平发展。

\section{本文主要研究内容与贡献}

为解决上述核心问题，本论文提出了一种新颖的混合训练与渲染框架。该框架的核心思想是，利用一个强大的可控视频扩散模型作为"生成式教师"，为作为"学生"的动态3DGS模型提供丰富且照片般逼真的监督信号，通过一种知识蒸馏（knowledge distillation）的范式，将生成模型的先验知识高效地迁移到实时渲染模型中。近期涌现的StreetCrafter等工作初步验证了这一思路的可行性：通过一个可控的视频扩散模型生成新视角的视频序列，然后将这些生成的图像作为额外的监督信号来优化一个动态3DGS表示\cite{yan2024street}。本论文将在该范式的基础上，进行更深入的探索与优化。

本文的主要研究内容与预期贡献可概括为以下四点：

\begin{enumerate}
\item \textbf{提出一种新颖的混合训练框架}：设计一种能够将扩散模型与3DGS表示进行更紧密耦合的训练架构。相较于简单的两阶段蒸馏流程，本研究将探索联合优化或端到端的训练策略，以期提升知识迁移的效率和最终模型的三维一致性。

\item \textbf{为动态场景构建先进的生成式先验}：研发一种新的扩散模型条件控制机制，该机制能够有效融合多模态传感器数据（如LiDAR点云、物体轨迹数据等），从而实现对场景几何结构与动态元素的精确控制，为生成高质量、时空连贯的动态场景提供坚实基础\cite{yan2024street}。

\item \textbf{实现高保真的视角外推能力}：本研究的核心目标之一是在新轨迹（如变道、转弯等）的视图合成任务上取得当前最优的性能。所提出的模型将能够在这些传统方法失效的场景中，生成照片级真实且几何一致的渲染结果，满足自动驾驶全方位模拟的核心需求\cite{shi2025drivex}。

\item \textbf{兼顾实时渲染与场景编辑功能}：最终输出的场景表示将完整保留3DGS的实时渲染特性。同时，得益于与生成先验的紧密联系，该表示将天然支持场景编辑应用，如动态物体的移除、替换或轨迹修改，进一步提升仿真系统的灵活性与实用性\cite{yan2024street}。
\end{enumerate}

\section{论文结构安排}

本论文的组织结构如下：

第二章将对相关工作进行系统性的回顾与评述，涵盖神经场景表示方法、用于三维内容生成的扩散模型，以及二者融合的研究现状。

第三章将详细阐述本文提出的模型架构与核心算法，包括混合训练框架的设计、动态生成先验的构建以及模型优化策略。

第四章将介绍实验设置、所用数据集以及评价指标，并展示本文方法在多个基准测试任务上与现有先进方法的定量与定性比较结果。

第五章将对实验结果进行深入的分析与讨论，包括消融实验、模型局限性分析以及对关键技术点的探讨。

第六章将对全文工作进行总结，并对未来可能的研究方向进行展望。
