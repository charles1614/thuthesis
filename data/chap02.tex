% !TeX root = ../thuthesis-example.tex

\chapter{相关工作}

本章系统性地回顾与本论文研究方向密切相关的理论基础和技术发展脉络。自动驾驶系统的构建需要在动态环境中实现准确的感知、预测和决策\cite{chen2024autonomous,hu2023planning,ma2024vision,kong2025multimodal}。为了在规模化条件下开发和评估此类系统,能够合成多样化、逼真且时序一致的4D场景的驾驶仿真器正变得日益关键。然而,收集和标注大规模真实世界驾驶数据集既昂贵又耗时,通常受到传感器覆盖范围、天气条件、遮挡和场景多样性的限制。为解决这些局限性,生成模型作为一种前景广阔的替代方案应运而生,使得能够可控且可扩展地创建用于感知和规划任务的合成驾驶场景。

基于扩散的模型在高保真图像和视频生成方面取得了显著成功,并正在积极探索应用于城市场景仿真。本章将从三个维度展开深入分析:首先综述驾驶场景生成技术的发展,包括视频生成和4D场景生成方法;其次回顾场景重建技术的演进,特别关注基于3D Gaussian Splatting的动态场景重建方法;最后介绍扩散强制技术的理论基础及其在视频生成中的应用,为本论文的技术创新提供理论支撑。

\section{驾驶场景生成技术}

近年来,基于扩散的方法\cite{ho2020denoising,ddpm2015,song2020score}已成为图像和视频生成领域的主导范式。基于这一成功,多项研究工作\cite{gao2024magicdrive,wen2024panacea,mao2025dreamdrive,wang2024drivedreamer,wang2024stag1,wen2024panacea,li2024hierarchical,gao2025vista,li2023bridging,wang2024driving}将扩散模型扩展到自动驾驶场景的生成和重建任务中。本节将从视频驾驶生成和4D驾驶生成两个层面系统性地分析该领域的技术进展。

\subsection{视频驾驶生成}

视频驾驶生成的核心目标是合成时序连贯且视觉逼真的街景视频序列。这一任务的挑战性在于需要同时满足多个约束条件:不仅要保证单帧图像的视觉质量,还要维持帧间的时序平滑性,同时响应各种控制信号如地图、轨迹和交通规则等。MagicDrive\cite{gao2024magicdrive}在这方面做出了开创性工作,通过编码多种3D几何信号作为条件输入,并采用跨视角注意力机制来增强帧间一致性,实现了高质量街景视频的生成。该方法的创新之处在于设计了多模态条件编码器,将BEV地图、3D边界框、实例掩码等结构化3D信息有效融入扩散过程的每一步,使得生成的视频能够在保持视觉真实性的同时维持几何一致性。此外,MagicDrive引入的跨视角注意力机制能够捕捉不同相机视角之间的空间关系,在一定程度上缓解了多视角不一致的问题。

在MagicDrive的基础上,后续工作进一步探索了提升时序连贯性和可控性的方法。Panacea\cite{wen2024panacea}提出了全景视频生成框架,其核心贡献在于设计了统一的潜在空间表示,能够同时控制多个视角的视频生成。该方法通过在潜在空间中共享全局场景表示,确保不同视角生成的视频在几何结构和语义内容上保持一致。这种设计使得系统能够生成360度环绕视角的连贯视频,为全方位场景理解提供了数据基础。DriveDreamer\cite{wang2024drivedreamer}则从另一个角度出发,构建了面向真实世界的世界模型。该方法不仅关注视频生成的视觉质量,更强调生成内容应符合真实世界的物理规律和交通规则。通过在大规模真实驾驶数据上学习场景的动态演化规律,DriveDreamer能够生成更加自然和合理的驾驶场景,包括车辆的加速减速、行人的移动模式、交通灯的变化等动态元素。

然而,这些视频生成方法存在一个根本性的局限:缺乏准确且连贯的3D空间表示。这一问题源于扩散模型本质上是在2D图像空间进行建模,虽然可以通过条件信号引入3D几何信息,但模型并不直接学习和维护完整的3D场景结构。因此,生成的视频虽然在单视角下具有较高的视觉质量,但往往无法保证多视角几何一致性。例如,当从不同视角观察同一场景时,可能出现物体位置不对应、尺度不一致、甚至几何结构矛盾等问题。这种几何不一致性在自动驾驶仿真中是不可接受的,因为感知算法和规划算法都依赖于精确的3D空间理解。

此外,现有视频生成方法通常与预定义的自车轨迹紧密耦合,只能生成沿着固定相机路径的视频序列。这种设计虽然简化了问题,但严重限制了系统的灵活性。在实际的自动驾驶仿真中,我们往往需要从任意视角观察场景,以评估感知系统在不同视角下的性能,或者可视化规划模块生成的多种可能轨迹。然而,这些视频生成方法无法支持新视角合成(Novel View Synthesis, NVS),一旦偏离训练时的相机分布,生成质量会急剧下降。这一局限性促使研究者们开始探索4D感知的场景生成方法,即不仅生成时序连贯的视频,还要构建完整的3D场景表示,以支持任意视角的渲染。

为了提升视频生成的时序连贯性和多视角一致性,一些工作从多个角度进行了深入探索\cite{li2024hierarchical,gao2025vista,li2023bridging,wang2024driving}。Li等人\cite{li2024hierarchical}提出了层次化时序上下文学习方法,其核心思想是在不同时间尺度上建模场景动态。该方法设计了多尺度的时序编码器,能够同时捕捉短期的局部运动(如车辆的瞬时速度变化)和长期的全局趋势(如整体交通流的演化)。通过这种层次化建模,系统能够生成在多个时间尺度上都保持连贯的视频序列。

VISTA\cite{gao2025vista}则着眼于构建通用的驾驶世界模型,目标是开发一个能够适应不同场景、不同天气条件、不同地理位置的统一生成框架。该方法通过在多样化的大规模数据集上进行训练,学习到了丰富的场景先验知识,包括道路结构、交通参与者的行为模式、光照变化等。VISTA的一个重要特点是其可控性,用户可以通过多种方式指定生成内容,如文本描述、轨迹规划、地图信息等。在多个基准测试中,VISTA展现了优异的泛化能力,能够生成在视觉质量和物理合理性上都接近真实数据的仿真场景。此外,Li等人\cite{li2023bridging}和Wang等人\cite{wang2024driving}分别从立体几何融合和多视角预测的角度,提出了提升几何一致性的新方法。

尽管这些工作从不同角度推动了视频驾驶生成技术的发展,但它们都未能从根本上解决3D几何一致性问题。其核心原因在于这些方法仍然是在2D视频空间进行生成,缺乏显式的3D场景表示作为约束。这促使研究者们转向4D场景生成方法,试图在生成过程中同时构建完整的3D几何结构。

\subsection{4D驾驶场景生成}

为了解决视频生成方法缺乏3D空间表示的问题,研究者们开始探索4D感知的场景生成方法。所谓4D生成,是指同时建模场景的三维空间结构和时间演化,生成包含完整几何信息的动态场景表示。与传统的2D视频生成相比,4D生成面临着更高的复杂度:不仅要求在时间维度上保持连贯性,还需要在三维空间中维持几何一致性,确保从任意视角渲染的结果都符合相同的3D结构。这一能力对于支持灵活的新视角合成至关重要,是构建高保真驾驶仿真器的必要条件。

4D场景生成的技术挑战是多方面的。首先是表示选择的问题:应该采用何种3D表示来同时满足几何准确性、渲染效率和生成灵活性的要求?其次是生成一致性的保证:如何确保生成的3D场景在不同视角和时刻都保持物理合理性和视觉连贯性?最后是计算效率的权衡:4D生成涉及的数据量和计算复杂度都远超2D视频生成,如何在可接受的计算成本下实现高质量的4D场景合成?针对这些挑战,研究者们提出了多种技术路线。

MagicDrive3D\cite{gao2024magicdrive3d}将视频生成与重建管线相结合,以合成4D无界场景。该方法采用两阶段策略:首先使用扩散模型生成多视角视频,然后通过神经辐射场(NeRF)或3D Gaussian Splatting (3DGS)进行场景重建。这种设计使得系统能够支持新视角渲染,但也引入了显著的计算开销,因为每个场景都需要进行逐场景优化。InfiniCube\cite{lu2024infinicube}引入了基于体素的中间表示,通过显式的3D体素网格来桥接视频生成和场景重建。该方法的优势在于能够更直接地控制3D几何结构,但体素表示的内存消耗和计算复杂度限制了其可扩展性。

UniScene\cite{li2025uniscene}利用占据网格生成LiDAR点云和视频帧,通过点云重投影和基于扩散的补全实现多模态一致性。该方法的创新之处在于统一了不同模态的生成过程,使得生成的LiDAR数据和相机图像在几何上保持一致。然而,占据网格的离散性限制了场景细节的表现能力,难以捕捉复杂的几何结构和纹理细节。

DiST-4D\cite{guo2025dist4d}通过生成深度图进一步改进了新视角重建能力。该方法采用解耦的时空扩散策略,分别建模空间几何和时序动态,然后通过度量深度监督来增强几何一致性。尽管在理论上具有吸引力,但该方法在推理过程中面临深度质量退化问题,严重影响了生成4D场景的时空一致性。深度估计的误差会在时间和空间上累积,导致生成的场景出现几何扭曲和视觉伪影。

最近的工作STAG-1\cite{wang2024stag1}和Gen3C\cite{ren2025gen3c}提出了以深度信息为桥梁的框架,在不进行显式逐场景优化的情况下实现场景生成和新视角合成。STAG-1利用预测的深度来实现4D一致的场景合成,通过将深度作为几何约束来指导视频生成过程。Gen3C则采用精确的相机控制策略,通过3D感知的世界一致性视频生成来支持任意视角渲染。这些方法展示了深度引导生成的潜力,但在推理阶段,预测深度的质量退化问题依然存在,这凸显了需要更统一且鲁棒的解决方案来确保整个生成过程中的几何保真度。

综上所述,现有的4D驾驶场景生成方法在几何一致性、计算效率和泛化能力之间面临权衡。基于逐场景优化的方法虽然能够生成高质量的场景,但计算成本过高,难以满足实时仿真的需求。基于深度引导的方法虽然提升了效率,但深度质量的退化问题限制了其实用性。这些局限性为本论文的研究提供了明确的改进空间。

\section{场景重建技术}

场景重建是构建高保真驾驶仿真器的核心技术之一。传统的场景重建方法依赖于显式的三维表示如点云或网格,而近年来基于神经表示的方法为场景重建带来了革命性的突破。本节重点关注3D Gaussian Splatting技术在自动驾驶场景重建中的应用和发展。

\subsection{神经辐射场与Gaussian Splatting}

神经辐射场(NeRF)的提出标志着隐式神经表示在场景重建领域的重大突破\cite{mildenhall2021nerf}。NeRF的核心思想是将三维场景建模为一个连续的五维函数$F: (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)$,该函数能够将三维空间位置$\mathbf{x} \in \mathbb{R}^3$和二维观测方向$\mathbf{d} \in \mathbb{S}^2$映射为RGB颜色$\mathbf{c} \in \mathbb{R}^3$和体积密度$\sigma \in \mathbb{R}^+$。通过体积渲染方程对沿射线的颜色和密度进行积分,NeRF能够生成任意视角的高质量图像。这种连续表示的优势在于其理论上的无限分辨率和天然的平滑性,使得NeRF在新视角合成质量上取得了前所未有的成功。

然而,NeRF的隐式表示也带来了显著的计算效率问题。由于需要对每条光线进行密集采样并多次查询神经网络,NeRF的渲染速度极慢,通常需要数秒才能生成一张图像。这一问题在大规模驾驶场景中尤为突出,因为城市级场景往往跨越数公里的范围,包含大量细节和动态元素。为了解决这一挑战,Block-NeRF采用了空间分割策略\cite{blocknerf2022},将大规模场景分解为多个重叠的子区域,每个子区域使用独立的NeRF模型进行建模。这种方法的优势在于能够并行训练和渲染不同的区域,同时通过精心设计的混合策略来确保区域边界的平滑过渡。Mega-NeRF\cite{meganeRF2022}进一步改进了这一思路,提出了基于八叉树的层次化分割策略,能够根据场景的几何复杂度自适应地调整分割粒度,从而更高效地处理城市级别的大规模场景。

在自动驾驶领域,将NeRF应用于动态场景建模面临着额外的挑战\cite{wu2023mars,yang2023emernerf}。动态驾驶场景不仅包含静态的道路、建筑等基础设施,还包含大量运动的车辆、行人等动态元素。如何在统一的框架中同时建模静态和动态成分,并确保它们在时空上的一致性,是一个复杂的问题。MARS\cite{wu2023mars}提出了实例感知的模块化仿真器,其核心思想是将场景分解为静态背景和多个动态物体实例。静态背景使用传统的NeRF建模,而每个动态物体则使用独立的实例级NeRF表示,并通过刚体变换参数来描述其运动轨迹。这种显式的动静分离策略不仅提升了重建效率,还使得系统能够方便地编辑和操控单个物体,为交互式仿真提供了基础。

EmerNeRF\cite{emernerf2023,yang2023emernerf}则采用了更加优雅的自监督学习方法来实现时空场景分解。该方法的关键创新在于设计了一个统一的神经场表示,能够同时建模静态场景和动态元素,无需依赖额外的实例分割标注。EmerNeRF通过引入流场(flow field)来表示场景的运动,并利用时序一致性约束来区分静态和动态成分。具体而言,对于静态区域,模型学习到零流场;对于动态区域,模型则学习物体的实际运动模式。通过这种方式,EmerNeRF实现了完全自监督的时空分解,能够从纯粹的多视角视频中学习到场景的4D表示,包括静态几何、动态物体及其运动轨迹。这种端到端的学习方式大大简化了系统的复杂度,提升了方法的通用性。

3D Gaussian Splatting (3DGS)的出现标志着从隐式到显式表示的重要范式转变\cite{kerbl2023gaussian,kerbl20233dgs}。与NeRF的连续函数表示不同,3DGS采用了显式的点云表示,将场景建模为大量三维高斯基元的集合。每个高斯基元由以下参数完整描述:三维位置$\boldsymbol{\mu} \in \mathbb{R}^3$定义其空间位置,3×3协方差矩阵$\boldsymbol{\Sigma}$描述其形状和朝向,不透明度$\alpha \in [0,1]$控制其透明度,以及颜色特征(通常采用球谐函数编码以支持视角相关的外观)。渲染时,3DGS通过可微分的点云splatting技术,将这些高斯基元按深度顺序混合到图像平面上,实现了高效且高质量的渲染。

这种显式表示方法带来了多个重要优势。首先是渲染效率的革命性提升:3DGS能够实现超过100帧每秒的实时渲染性能,比NeRF快了几个数量级。这得益于高斯基元的显式性质——渲染时只需要处理投影到当前视角的可见高斯,而不需要像NeRF那样对整个场景进行密集采样。其次是优化效率的显著改善:3DGS的参数可以直接通过光度损失进行端到端优化,收敛速度远快于NeRF的隐式表示。此外,3DGS还支持自适应的密度控制,能够在优化过程中动态增删高斯基元,自动调整场景表示的复杂度。这些特性使得3DGS成为构建实时高保真驾驶仿真器的理想选择,为实际应用部署奠定了坚实基础。

\subsection{动态驾驶场景重建}

在动态驾驶场景中应用3DGS面临着独特的挑战,需要同时处理静态环境和动态物体的建模。多项研究\cite{zhou2024drivinggaussian,yan2024street,yan2024streetgaussians,chen2023pvg,chen2023pvg_arxiv}探索了3DGS在自动驾驶场景中的应用。

DrivingGaussian\cite{zhou2024drivinggaussian}针对环绕视角的动态自动驾驶场景,提出了复合高斯溅射方法。该方法采用动静分离策略,通过不同的高斯表示来分别建模静态背景和动态前景,并通过刚体变换来描述动态物体的运动。Street Gaussians将3DGS扩展到动态城市场景,采用周期性振动高斯来捕捉重复性运动模式,如树叶的摆动和车辆的周期性运动\cite{yan2024street,yan2024streetgaussians}。Periodic Vibration Gaussian (PVG)\cite{chen2023pvg,chen2023pvg_arxiv}进一步精细化了动态建模,通过学习周期性形变参数来表示场景中的规律性动态。

OmniRe\cite{chen2024omnire}提出了全方位城市场景重建方法,能够处理大规模室外场景的重建任务。EGSRAL\cite{huo2025egsral}结合了3DGS和自动标注,为大规模驾驶场景提供了增强的渲染和标注能力。这些方法在重建质量和渲染效率上取得了显著进展,但仍面临泛化能力不足的问题——它们通常需要针对每个场景进行逐场景优化,限制了在未见场景上的应用。

\subsection{可泛化的高斯重建}

为了克服逐场景优化的局限性,最近的研究\cite{tian2024drivingforward,lu2024drivingrecon,yang2024storm}探索了前馈式的可泛化高斯重建方法。这类方法的核心思想是训练一个通用的网络,能够直接从输入图像生成高斯表示,无需针对特定场景进行优化。

DrivingForward\cite{tian2024drivingforward}提出了前馈式3D高斯溅射重建方法,能够从灵活的环绕视角输入进行驾驶场景重建。该方法通过学习从多视角图像到高斯参数的直接映射,实现了实时的场景重建。DrivingRecon\cite{lu2024drivingrecon}构建了大规模4D高斯重建模型,通过在大规模数据集上训练来获得强大的泛化能力。STORM\cite{yang2024storm}提出了针对大规模室外场景的时空重建模型,能够处理动态物体和相机运动带来的复杂时空变化。

这些可泛化方法的出现为实时4D重建提供了可能,但它们主要关注重建任务本身,缺乏与生成模型的深度融合。如何将预训练生成模型的先验知识有效迁移到4D重建中,仍然是一个有待解决的开放问题。此外,现有方法在处理遮挡、光照变化和稀疏视角等挑战性场景时,重建质量仍有较大提升空间。

更多的泛化高斯重建工作\cite{yang2023deformable,tang2024lgm,zhang2024gslrm,charatan2024pixelsplat,chen2024mvsplat,ren2024scube}从不同角度探索了提升重建泛化能力的方法。Deformable 3D Gaussians\cite{yang2023deformable}通过引入可变形机制来处理非刚性变形。LGM\cite{tang2024lgm}和GS-LRM\cite{zhang2024gslrm}构建了大规模多视角高斯模型,通过在大规模数据集上预训练来提升泛化性。PixelSplat\cite{charatan2024pixelsplat}和MVSplat\cite{chen2024mvsplat}探索了从稀疏图像对进行高效重建的方法。SCube\cite{ren2024scube}提出了基于体素溅射的即时大规模场景重建方法。

\section{扩散强制技术}

扩散强制(Diffusion Forcing)技术代表了扩散模型在序列生成领域的重要理论突破。这一技术的提出源于对传统扩散模型局限性的深刻认识。在标准的扩散模型中,所有数据维度(对于视频而言即所有帧)被施加相同水平的噪声,并通过统一的去噪过程恢复。这种设计虽然简单,但忽略了序列数据的时序依赖特性——前面的帧应该为后续帧的生成提供条件约束,而不应该被同等程度的噪声破坏。扩散强制通过为每帧独立变化噪声水平,实现了更灵活和合理的序列建模机制。

具体而言,扩散强制允许对序列的不同部分施加不同程度的噪声破坏。例如,在视频生成中,可以对历史帧施加较低的噪声(甚至保持完全清晰),而对未来需要生成的帧施加较高的噪声。这种设计使得模型在去噪过程中能够充分利用历史信息作为条件,避免了传统方法中历史信息也被噪声破坏的问题。这一看似简单的改变,实际上统一了自回归生成和扩散模型两种范式,为视频生成提供了更强大的建模能力。

\subsection{扩散强制的理论基础}

Diffusion Forcing\cite{chen2024diffusion}在因果状态空间模型中引入了逐帧噪声调度机制,实现了灵活的去噪过程,并统一了下一标记预测与扩散的范式。该方法的理论创新在于将序列生成问题重新形式化为条件扩散过程,通过控制不同时间步的噪声水平来实现对生成过程的精细控制。这种设计使得模型能够更好地处理长序列生成中的误差累积问题,提升了生成质量和稳定性。

CausVid\cite{yin2024causvid}将扩散强制扩展到因果Transformer架构,通过因果注意力机制缓解自回归视频生成中的误差累积问题,从而提升了长视频合成的效率和稳定性。该方法的核心思想是在自回归生成过程中引入扩散去噪,使得每一步生成都能够利用扩散模型的强大先验来修正累积误差。实验结果表明,这种混合架构在保持生成质量的同时显著提升了长视频生成的稳定性。

\subsection{非因果扩散强制与历史引导}

最近,Song等人\cite{song2025history}提出了非因果扩散强制Transformer (DFoT),通过移除因果约束来支持任意长度的历史条件。该方法引入了历史引导(History Guidance)机制,通过显式利用历史信息来增强时序一致性,支持鲁棒的长序列生成。非因果设计使得模型能够同时访问过去和未来的信息,从而更好地维持全局一致性。在视频生成任务中,这种设计显著提升了模型处理复杂时序依赖关系的能力,在处理可变长度上下文和复杂时序动态方面表现优于先前方法。

Self Forcing\cite{huang2025self}进一步扩展了这一思想,通过在训练过程中模拟推理过程来训练自回归视频扩散模型。该方法采用KV缓存机制进行自回归展开,使得训练过程更接近实际的推理场景,从而减小训练-测试差距。这种设计理念与课程学习相呼应,通过逐步增加任务难度来提升模型的泛化能力。

\subsection{扩散强制在4D生成中的应用潜力}

虽然扩散强制技术在视频生成领域展现了强大的潜力,但其在4D场景生成中的应用尚未得到充分探索。传统的扩散强制主要关注时序维度的噪声调度,而4D场景生成还需要考虑空间几何的一致性。如何将自适应噪声调度的思想扩展到空间域,根据几何不确定性动态调整去噪强度,是一个值得深入研究的方向。

本论文受到扩散强制技术的启发,提出了针对4D场景生成的Stereo Forcing方法。与传统扩散强制关注时序维度不同,Stereo Forcing关注空间几何维度,通过向不同深度区域注入不同水平的噪声,利用生成模型的几何补全能力来恢复跨帧一致且准确的深度信息,从而提升4D时空连贯性。这一创新为将扩散强制思想应用于几何感知的场景生成开辟了新的方向。

\section{融合生成与重建的前沿探索}

除了上述三个主要技术方向,还有一些工作\cite{zhao2024drivedreamer4d,zhao2025drivedreamer4d_cvpr,ni2025recondreamer,zhao2025recondreamerpp,fan2024freesim,wei2025omniscene}探索了融合生成模型与重建方法的新范式。

DriveDreamer4D代表了数据机器范式的典型实现,利用世界模型生成额外的训练数据来改善4DGS模型的性能\cite{zhao2024drivedreamer4d,zhao2025drivedreamer4d_cvpr}。该方法通过增加训练数据的视点覆盖范围来缓解视点外推问题,展示了生成模型作为数据增强工具的潜力。然而,合成数据与真实数据之间的领域差异可能导致模型学习到错误的先验知识,且这种间接的知识迁移方式效率相对有限。

ReconDreamer提出了在线修复范式,将生成模型作为实时修复器嵌入训练循环\cite{ni2025recondreamer}。该方法在每个训练迭代中使用预训练的修复模型对渲染结果进行改善,实现了更直接的知识迁移。ReconDreamer++进一步协调了生成模型和重建模型,通过联合优化来提升驾驶场景表示的质量\cite{zhao2025recondreamerpp}。这些方法展示了生成-重建融合的潜力,但较高的训练复杂度和潜在的误差累积风险限制了其实用性。

FreeSim探索了驾驶场景中的自由视角相机仿真,支持从任意视角观察和渲染场景\cite{fan2024freesim}。Omni-Scene提出了全向高斯表示,用于以自我为中心的稀疏视角场景重建\cite{wei2025omniscene}。这些方法从不同角度推动了场景重建和渲染技术的发展,为构建更灵活的驾驶仿真系统提供了新的思路。

在深度估计和语义理解方面,多项工作\cite{xie2021segformer,liu2024depthlab,wei2023surrounddepth,wang2024freevs}提供了重要的技术支撑。SegFormer\cite{xie2021segformer}提出了简单高效的语义分割Transformer架构。DepthLab\cite{liu2024depthlab}探索了从局部到完整的深度补全方法。SurroundDepth\cite{wei2023surrounddepth}通过交织环绕视角实现自监督多相机深度估计。FreeVS\cite{wang2024freevs}在自由驾驶轨迹上实现了生成式视角合成。这些技术为驾驶场景的几何理解和视角合成提供了重要基础。

此外,pixelNeRF\cite{yu2021pixelnerf}、DUSt3R\cite{wang2023dust3r}和MVSGaussian\cite{liu2024mvsgaussian}等方法探索了从少量图像进行场景重建的可能性,为提升重建效率提供了新的思路。DriveArena\cite{yang2024drivearena}构建了闭环生成仿真平台,将生成模型与自动驾驶系统深度集成,展示了生成技术在实际应用中的潜力。

在数据集和基准测试方面,Waymo Open Dataset\cite{sun2020waymo}、nuScenes\cite{caesar2020nuscenes}和ASAP\cite{wang2022asap}等大规模数据集为驾驶场景生成和重建研究提供了重要支撑。在泛化性研究方面,Wang等人\cite{wang2023domain}探索了多视角3D目标检测的领域泛化问题,Lu等人\cite{lu2024generalizable}研究了多相机3D目标检测的泛化能力,这些工作为提升模型在不同场景和条件下的鲁棒性提供了重要见解。

\section{本章小结}

本章通过系统性的文献回顾,全面梳理了驾驶场景生成、场景重建和扩散强制技术的研究现状,为本论文的技术创新奠定了坚实的理论基础。

在驾驶场景生成方面,研究领域经历了从2D视频生成到4D场景生成的重要演进。早期的视频生成方法主要关注视觉质量和时序连贯性,通过扩散模型在图像空间进行建模,取得了令人印象深刻的生成效果。然而,这些方法的根本局限在于缺乏显式的3D几何表示,导致生成的内容虽然视觉逼真,但无法保证多视角几何一致性,严重限制了其在自动驾驶仿真中的应用。为了解决这一问题,研究者们开始探索4D场景生成方法,试图在生成过程中同时构建完整的3D结构。然而,现有的4D生成方法仍面临多重挑战:基于逐场景优化的方法计算成本过高,无法满足实时仿真需求;基于深度引导的方法虽然提升了效率,但深度质量退化问题限制了生成场景的时空一致性。这些局限性表明,在几何一致性、计算效率和泛化能力之间找到合理的平衡点,仍然是一个亟待解决的开放性问题。

在场景重建技术方面,从NeRF到3DGS的演进体现了从隐式到显式表示的范式转变。NeRF通过连续函数表示实现了高质量的新视角合成,但其计算效率问题限制了在大规模动态场景中的应用。3DGS的出现革命性地提升了渲染效率,使实时高质量重建成为可能。然而,如何将3DGS扩展到可泛化的4D重建,即无需逐场景优化就能处理未见场景,仍然是一个重大挑战。现有的可泛化重建方法虽然在单场景重建上取得了进展,但它们主要关注重建任务本身,缺乏与生成模型的深度融合。如何有效利用大规模预训练生成模型的先验知识来增强重建的鲁棒性,特别是在处理遮挡、光照变化和稀疏视角等挑战性场景时,是一个值得深入研究的方向。

扩散强制技术为序列生成提供了新的理论框架,其核心思想是通过自适应噪声调度来更好地利用历史信息。这一技术在视频生成领域已经展现了强大的潜力,但其在4D场景生成中的应用尚未得到充分探索。传统的扩散强制主要关注时序维度的噪声调度,而4D场景生成不仅需要考虑时序一致性,还需要维护空间几何的准确性。如何将自适应噪声调度的思想从时序维度扩展到空间几何维度,根据场景的几何不确定性动态调整去噪强度,是一个富有前景的研究方向。

综合以上分析,本论文识别出了现有技术的三个核心局限:一是缺乏端到端的4D生成框架,现有方法通常将生成和重建分离处理,导致知识迁移效率低下;二是缺乏几何感知的生成引导机制,无法有效利用几何不确定性信息来提升生成质量;三是缺乏可泛化的前馈重建能力,大多数方法依赖逐场景优化,限制了实际应用的可扩展性。这些技术挑战和研究空白为本论文的研究提供了明确的改进方向和创新空间。通过构建统一的端到端4D生成框架,设计基于几何不确定性的Stereo Forcing机制,开发可泛化的前馈重建方法,本论文旨在突破现有技术的系统性局限,为高保真驾驶场景生成技术在自动驾驶仿真中的实际应用提供更加可靠和高效的技术支撑。
