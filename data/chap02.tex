% !TeX root = ../thuthesis-example.tex

\chapter{相关工作}

本章旨在对自动驾驶场景重建领域的相关技术进行全面而深入的综述。内容将围绕三个核心主题展开：首先，回顾用于自动驾驶场景的神经表示方法，重点剖析从隐式的神经辐射场（NeRF）到显式的三维高斯溅射（3DGS）的技术演进路径及其在动态场景建模中的扩展；其次，探讨降噪扩散模型在三维内容生成领域的应用，分析其原理与主流实现范式；最后，聚焦于场景表示与生成模型的融合技术，为本论文的研究工作提供清晰的学术定位。

\section{自动驾驶场景的神经场景表示方法}

神经场景表示（Neural Scene Representation）旨在利用神经网络来参数化三维场景，以实现高质量的渲染和重建。近年来，这一领域经历了从隐式表示到显式表示的重大转变。

\subsection{隐式神经辐射场 (Implicit Neural Radiance Fields - NeRFs)}

自2021年被提出以来，神经辐射场（NeRF）迅速成为三维场景表示的里程碑式技术\cite{mildenhall2021nerf}。其核心思想是使用一个简单的多层感知机（MLP）来隐式地学习一个连续的五维函数$F_{\Theta}: (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)$，该函数将一个三维空间点坐标$\mathbf{x} = (x, y, z)$和二维观测方向$\mathbf{d} = (\theta, \phi)$映射到该点的RGB颜色值$\mathbf{c}$和体密度$\sigma$\cite{mildenhall2021nerf}。通过沿相机光线进行密集采样，并利用经典的体积渲染方程对采样点的颜色和密度进行积分，NeRF能够从一组静态二维图像中合成具有高度真实感的新视角图像。

然而，原始的NeRF专为静态、有界的、以物体为中心的场景设计，直接应用于广阔、无界且动态的自动驾驶场景时面临巨大挑战。为此，研究者们提出了一系列改进方案。为了处理大尺度场景，一些工作引入了场景分解策略，例如将场景划分为前景和背景，或采用分块建模的方式\cite{chen2025snerf}。为了提升几何精度，许多方法开始融合激光雷达（LiDAR）等主动传感器的稀疏但精确的深度信息作为额外的几何约束或监督信号，如Urban Radiance Field、S-NeRF++和NeuRAD等工作\cite{chen2025snerf}。其中，S-NeRF++通过利用稀疏的LiDAR数据来监督NeRF的深度预测，有效解决了大场景重建中的几何不确定性问题\cite{chen2025snerf}。NeuRAD则进一步将相机、LiDAR和雷达数据统一到一个联合的神经特征场中进行建模，增强了多模态数据的一致性\cite{tonderski2024neurad}。

尽管这些改进极大地提升了NeRF在自动驾驶场景中的表现，但它们未能触及NeRF最根本的性能瓶颈。无论场景参数化如何优化，基于体积渲染的逐光线密集采样与MLP查询机制所带来的高昂计算开销始终存在。这使得NeRF及其变体在渲染速度上难以满足自动驾驶仿真等需要实时交互的应用需求，为其大规模部署带来了根本性的障碍\cite{chen2025snerf}。

\subsection{显式三维高斯溅射 (Explicit 3D Gaussian Splatting - 3DGS)}

为了彻底解决NeRF的渲染效率问题，3D高斯溅射（3DGS）于2023年被提出，并迅速成为实时神经渲染领域的新范式\cite{kerbl2023gaussian}。与NeRF的隐式表示不同，3DGS采用了一种完全显式的、基于点的场景表示方法。它将三维场景建模为数以百万计的三维高斯椭球体的集合，每个高斯体由其中心位置$\mathbf{\mu}$、3x3协方差矩阵$\mathbf{\Sigma}$（描述其形状和旋转）、不透明度$\alpha$以及一组球谐函数（Spherical Harmonics）系数（描述其随视角变化的颜色）共同定义\cite{kerbl2023gaussian}。

3DGS的渲染过程摒弃了耗时的光线步进，转而采用一种高效的可微分光栅化管线。在渲染时，所有三维高斯体被直接投影到二维图像平面上，形成一系列二维高斯"溅射点"（splats）。这些溅射点随后通过基于瓦片（tile-based）的光栅化器进行高效排序和混合，最终合成目标图像。由于整个渲染流程高度并行化且可微分，3DGS不仅能够实现超实时（远超30 fps）的高分辨率渲染，还能通过反向传播梯度来端到端地优化所有高斯体的参数\cite{kerbl2023gaussian}。这种显式表示还带来了额外的优势，例如场景更易于解释、编辑和存储\cite{kerbl2023gaussian}。

3DGS的出现，为构建高性能的自动驾驶仿真系统扫清了渲染速度这一关键障碍。

\subsection{动态场景的3DGS扩展方法}

原始3DGS主要针对静态场景。为了将其应用于普遍存在动态物体的自动驾驶环境，研究者们提出了一系列扩展方法。这些方法可以根据其对监督信息的需求，大致分为监督方法和自监督/统一方法两大类。

\textbf{监督方法：} 这类方法通常依赖于外部提供的、通常是人工标注的先验信息来区分和建模动态物体。代表性工作如StreetGaussian和DrivingGaussian，它们利用预先获取的车辆三维边界框（3D Bounding Boxes）轨迹，将场景显式地分解为静态背景和动态前景两部分\cite{lin2024drivinggaussian}。静态背景由一组静态3D高斯体表示，而每个动态物体则由另一组附着于其自身坐标系的3D高斯体表示，这些高斯体随着物体的运动轨迹进行刚性变换。这种分解-建模的策略虽然直观有效，但其最大的局限性在于对高质量标注数据的严重依赖。获取精确的、逐帧的三维边界框轨迹成本高昂，且在复杂场景中难以保证其准确性，这极大地限制了此类方法的可扩展性和在野外数据上的应用能力\cite{sun2025splatflow}。

\textbf{自监督与统一方法：} 为了摆脱对昂贵标注的依赖，研究者们开始探索从数据自身中学习动态信息的方法。这一研究方向标志着动态场景建模向着更通用、更具可扩展性的目标迈进。Periodic Vibration Gaussian (PVG) 提出了一种优雅的统一表示模型\cite{chen2023pvg}。它摒弃了将场景硬性分割为静态和动态两部分的想法，而是为每个高斯体引入了基于周期性振荡的时间动态属性。通过这种方式，静态物体可以被视为振动幅度为零的特例，而动态物体的运动则通过学习到的振动参数（如速度、相位等）来描述。PVG通过一个统一的数学公式，实现了对场景中所有元素的无差别建模，并且能够在没有边界框标注的情况下，自监督地学习出物体的动态特性\cite{chen2023pvg}。

SplatFlow 则采用了另一种自监督的思路\cite{sun2025splatflow}。它引入了一个"神经运动流场"（Neural Motion Flow Field, NMFF）来学习场景中任意点在时间维度上的运动轨迹。该运动流场首先在LiDAR点云序列上进行预训练，然后用于指导场景的分解。通过分析点的运动幅度，SplatFlow能够自监督地将场景点云划分为静态部分和动态部分，并分别用3D高斯体和4D高斯体（即时变的高斯体）进行建模。这种方法同样无需边界框标注，而是从数据内在的时空一致性中推断出动态信息\cite{sun2025splatflow}。

从依赖强监督的StreetGaussian，到探索统一表示的PVG，再到引入运动流场的SplatFlow，我们可以清晰地观察到一条迈向"无标注动态建模"的技术演进轨迹。这一趋势反映了学术界对于方法可扩展性和对"野外"数据适应性的高度重视。一个理想的动态场景重建系统，应当能够直接从原始的多模态传感器数据中学习，而无需依赖繁琐且昂贵的人工标注流程。本论文的研究工作也将遵循这一趋势，旨在提出一种仅依赖于原始传感器数据即可实现高质量动态场景重建与生成的方法。

为了更清晰地梳理和对比这些先进的动态场景3DGS重建方法，表~\ref{tab:dynamic-3dgs-comparison}总结了它们的关键特性、所需监督、以及主要优缺点。

\begin{table}
  \centering
  \caption{动态场景3DGS重建代表性方法对比}
  \label{tab:dynamic-3dgs-comparison}
  \begin{tabular}{p{2.5cm}p{3cm}p{2cm}p{1.5cm}p{1.5cm}p{3cm}}
    \toprule
    方法 & 核心思想 & 所需监督 & 视角外推能力 & 实时渲染 & 主要局限性 \\
    \midrule
    StreetGaussian\cite{yan2024street} & 将场景分解为静态/动态高斯体 & 三维边界框 & 有限 & 是 & 依赖昂贵的人工标注 \\
    PVG\cite{chen2023pvg} & 采用周期性振荡动态的统一表示 & 自监督（时序） & 中等 & 是 & 对复杂的非周期性运动建模能力可能受限 \\
    SplatFlow\cite{sun2025splatflow} & 通过神经运动流场进行自监督分解 & 自监督（运动） & 中等 & 是 & 运动流场估计可能引入额外复杂度和计算开销 \\
    StreetCrafter\cite{yan2025streetcrafter} & 将LiDAR条件化的视频扩散先验蒸馏至3DGS & 传感器数据（LiDAR, 轨迹） & 强 & 是（蒸馏后） & 两阶段流程；扩散模型生成过程缓慢 \\
    [本论文方法] & [待定] & [传感器数据] & [目标：强] & [目标：是] & [旨在克服两阶段流程的低效性，提升融合效果] \\
    \bottomrule
  \end{tabular}
\end{table}

\section{用于三维内容生成的扩散模型}

降噪扩散模型已成为生成式人工智能的支柱技术，其强大的建模能力正被越来越多地应用于三维内容的生成。

\subsection{降噪扩散模型原理}

降噪扩散概率模型（Denoising Diffusion Probabilistic Models, DDPM）是一类深度生成模型，其核心机制包含两个过程：前向扩散过程和反向去噪过程\cite{ho2020denoising}。

\textbf{前向过程（Forward Process）：} 这是一个固定的、无学习参数的马尔可夫链。它从一个真实数据样本$\mathbf{x}_0$开始，在$T$个离散的时间步中，逐步向其添加预设方差的高斯噪声。经过$T$步后，原始数据$\mathbf{x}_0$几乎完全转化为一个标准高斯噪声分布$\mathcal{N}(0, \mathbf{I})$。

\textbf{反向过程（Reverse Process）：} 这是一个参数化的马尔可夫链，其目标是学习前向过程的逆过程。它从一个纯高斯噪声$\mathbf{x}_T$开始，通过一个神经网络（通常是U−Net架构）在每个时间步$t$预测并去除噪声，逐步将噪声还原为真实数据样本$\mathbf{x}_0$。通过在大量数据上训练这个去噪网络，模型能够隐式地学习到整个数据流形的分布。

\subsection{基于二维先验的三维感知生成}

由于高质量的三维数据集相对稀缺，而二维图像数据则极为丰富，当前主流的三维内容生成范式是利用在海量图像上预训练好的大型二维扩散模型（如Stable Diffusion）作为强大的先验知识。这种范式通常通过一种名为"分数蒸馏采样"（Score Distillation Sampling, SDS）的优化技术来实现\cite{ho2020denoising}。其基本思想是，在优化一个可微的三维场景表示（如NeRF或3DGS）时，在每个迭代步骤中，随机渲染一个视角下的二维图像，并利用预训练的二维扩散模型来评估该图像的"真实性"。扩散模型会提供一个梯度信号，指导三维表示朝着生成更"合理"的渲染结果的方向进行更新。这种方法已被成功应用于文本到三维（text-to-3D）生成、三维场景风格化等任务\cite{kerbl2023gaussian}。

然而，其核心挑战在于保证多视角一致性。由于二维先验本身不具备三维几何知识，它对每个视角的评估是独立的。这常常导致优化过程陷入一种" Janus（双面神）问题"，即生成的物体从每个单独的视角看都合理，但组合在一起时却不构成一个连贯的三维实体，从而产生几何或纹理上的不一致性\cite{kerbl2023gaussian}。

\subsection{三维场景的直接生成建模}

为了从根本上解决多视角一致性问题，另一条研究路径是直接在三维数据上训练扩散模型。这种方法虽然能够保证生成的三维模型的几何有效性，但面临着三维数据稀缺和计算成本高昂的双重挑战\cite{ho2020denoising}。

为了使直接三维生成变得可行，一个关键的技术是在一个压缩的潜空间（latent space）中进行扩散过程。L3DG（Latent 3D Gaussian Diffusion）是这一方向的杰出代表\cite{rossle2024l3dg}。L3DG的流程分为两步：首先，训练一个矢量量化变分自编码器（VQ-VAE），将一个完整的三维高斯溅射场景压缩到一个紧凑的、离散的潜空间网格中；然后，在这个高效的潜空间中训练一个扩散模型，使其学会从噪声生成新的潜空间表示。在推理阶段，通过潜空间扩散模型生成一个新的潜码，再由VQ-VAE的解码器将其恢复为一个完整的、可实时渲染的3DGS场景\cite{rossle2024l3dg}。L3DG展示了一种将3DGS与扩散模型深度结合的强大范式，为生成大规模、高质量的三维场景开辟了新的道路。

\section{场景表示与生成模型的融合}

将3DGS的实时渲染能力与扩散模型的生成能力相结合，是当前神经渲染领域最前沿、最具潜力的研究方向之一。这种融合的核心思想，是将强大的生成模型作为一种外部的、可泛化的"知识源"，用以指导或增强三维场景表示的优化过程。

\subsection{蒸馏范式：生成式先验作为监督信号}

目前，最成功的融合范式可以被形象地描述为一种"教师-学生"的知识蒸馏模型。在这个模型中，大型、强大的扩散模型扮演"教师"的角色。这位"教师"虽然知识渊博（因为它在海量数据上预训练，拥有丰富的世界先验），但其"授课"（生成图像）过程缓慢且计算昂贵\cite{ho2020denoising}。而3DGS模型则扮演"学生"的角色。这位"学生"虽然轻量、敏捷（能够实时渲染），但其自身知识有限，尤其是在面对未曾见过的场景（新视角）时表现不佳\cite{chen2023pvg}。

知识蒸馏的过程，就是让"教师"为"学生"生成大量的、高质量的"学习资料"。具体到自动驾驶场景重建中，StreetCrafter等工作提供了一个经典的实现流程\cite{shi2025drivex}：

\begin{enumerate}
\item \textbf{条件化与控制：} 首先，利用车载传感器数据，特别是LiDAR点云，来构建对视频扩散模型的强几何约束。通过将着色后的LiDAR点云渲染到图像空间，形成一个密集的、像素级的条件图。这种条件化方式为扩散模型提供了精确的相机位姿和场景几何结构信息，使其能够生成与真实世界几何高度一致的视频内容\cite{yan2024street}。

\item \textbf{可控视频生成：} 在强几何条件的引导下，微调后的视频扩散模型能够为任意给定的新相机轨迹（如模拟车辆变道或转弯）生成高质量、照片级真实且时空连贯的视频序列\cite{yan2024street}。这些生成的视频内容，填补了原始数据中不存在的视角空白。

\item \textbf{知识蒸馏至3DGS：} 这些由扩散模型生成的、既逼真又几何正确的视频帧，被用作高质量的"伪真值"（pseudo ground truth）数据。随后，一个动态3DGS模型以这些生成的图像作为监督信号进行从头优化\cite{yan2024street}。

\item \textbf{最终成果：} 经过这个蒸馏过程，最终的3DGS模型（"学生"）成功地"吸收"了扩散模型（"教师"）的强大生成能力，尤其是在视角外推方面的性能得到极大提升。同时，它完整地保留了自身作为显式表示的实时渲染特性\cite{yan2024street}。
\end{enumerate}

扩散模型的生成过程虽然缓慢（StreetCrafter报告的生成速度约为0.2 fps\cite{yan2025streetcrafter}），但这是一个一次性的、离线的"烘焙"步骤。一旦蒸馏完成，最终的3DGS模型便可以进行高效的实时渲染。包括DriveX和ReconDreamer++在内的其他工作也遵循了类似的范式，即利用生成先验来创造伪真值数据，以增强三维表示的质量和泛化能力，这证明了该研究方向的重要性和有效性\cite{shi2025drivex}。

\section{本章小结与研究切入点}

本章系统回顾了自动驾驶场景重建与生成的相关技术。通过梳理，我们识别出几条清晰的技术演进脉络：在场景表示层面，为了满足实时性需求，技术重心已从慢速的隐式NeRF表示迁移至快速的显式3DGS表示。在动态建模层面，3DGS正从依赖强监督（如边界框）的方法，向更具扩展性的自监督方法演进。在内容生成层面，扩散模型已确立其作为最先进生成工具的地位。一个新兴且充满前景的研究范式已经出现，即融合3DGS的重建能力与扩散模型的生成能力，其中以StreetCrafter为代表的知识蒸馏方法取得了初步成功。

然而，在肯定现有进展的同时，我们也必须审视当前最先进的融合方法存在的局限性，这些局限性正是本论文的研究切入点：

\begin{enumerate}
\item \textbf{两阶段流程的低效性：} 以StreetCrafter为代表的方法采用的是一种严格的序贯流程：先生成所有视频帧，再用这些帧训练3DGS模型。这个过程不仅耗时，而且两个阶段之间缺乏信息反馈。一个端到端或联合的训练框架，有可能通过允许梯度在生成模型和场景表示之间流动，来提升整体的训练效率和最终模型的一致性。

\item \textbf{对特定条件的依赖：} 当前方法高度依赖LiDAR作为主要的几何条件源。虽然这种方式效果显著，但其普适性有待商榷。在LiDAR数据稀疏、噪声较大或传感器配置不同的情况下，模型的性能可能会下降。

\item \textbf{潜在的伪影传播：} 知识蒸馏过程并非完美无瑕。扩散模型生成的内容中可能存在的微小不一致性或伪影，有可能会在优化过程中被"烘焙"到最终的3DGS模型中。一个更紧密集成的框架或许能通过实时反馈来缓解这一问题。
\end{enumerate}

综上所述，本论文的研究工作将立足于当前的技术前沿，直面上述挑战。本文旨在提出一个更为高效、鲁棒且紧密耦合的新型框架，以更优的方式实现3D高斯溅射与扩散模型的协同增效，从而在自动驾驶的实时、高保真仿真领域，推动技术边界向前迈进。这为第三章将要详细阐述的本论文方法论奠定了坚实的基础。
